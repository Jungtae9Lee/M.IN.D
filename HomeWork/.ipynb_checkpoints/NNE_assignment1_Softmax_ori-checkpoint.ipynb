{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1. Softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as pickle\n",
    "import time\n",
    "from YourAnswer import naive_softmax_loss, vectorized_softmax_loss\n",
    "from YourAnswer import Softmax\n",
    "\n",
    "# set default plot options\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up input preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use CIFAR-10 dataset, which has ten classes. In usual, you need to download it from website. But you don't need to worry, because we already downloaded it for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load all of CIFAR10 dataset.\n",
    "def load_CIFAR10(root):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(root, 'data_batch_%d' % (b, ))\n",
    "        with open(f, 'rb') as f:\n",
    "            datadict = pickle.load(f, encoding='latin1')\n",
    "            X = datadict['data']\n",
    "            Y = datadict['labels']\n",
    "            X = X.reshape(10000,3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
    "            Y = np.array(Y)\n",
    "        #X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X,Y\n",
    "    \n",
    "    f=os.path.join(root, 'test_batch')\n",
    "    with open(f, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding='latin1')\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        Xte = X.reshape(10000,3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Yte = np.array(Y)\n",
    "        \n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_CIFAR10_data():\n",
    "    # 1. Load the raw data\n",
    "    X_tr, Y_tr, X_te, Y_te = load_CIFAR10('./../cifar-10-batches-py')\n",
    "    \n",
    "    # 2. Divide the data\n",
    "    X_val, Y_val = X_tr[49000:], Y_tr[49000:]\n",
    "    X_tr, Y_tr = X_tr[:49000], Y_tr[:49000]\n",
    "    X_te, Y_te = X_te[:1000], Y_te[:1000]\n",
    "\n",
    "    # 3. Preprocess the input image\n",
    "    X_tr = np.reshape(X_tr, (X_tr.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0],-1))\n",
    "    X_te = np.reshape(X_te, (X_te.shape[0],-1))\n",
    "    \n",
    "    # 4. Normalize the data (subtract the mean image)\n",
    "    mean_img = np.mean(X_tr, axis = 0)\n",
    "    X_tr -= mean_img\n",
    "    X_val -= mean_img\n",
    "    X_te -= mean_img\n",
    "\n",
    "    # 5. Add bias and Transform into columns\n",
    "    X_tr = np.hstack([X_tr, np.ones((X_tr.shape[0],1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0],1))])\n",
    "    X_te = np.hstack([X_te, np.ones((X_te.shape[0],1))])\n",
    "    \n",
    "    return X_tr, Y_tr, X_val, Y_val, X_te, Y_te, mean_img\n",
    "\n",
    "\n",
    "\n",
    "X_tr, Y_tr, X_val, Y_val, X_te, Y_te, mean_img = get_CIFAR10_data()\n",
    "print ('Train data shape : %s,  Train labels shape : %s' % (X_tr.shape, Y_tr.shape))\n",
    "print ('Validatoin data shape : %s,  Validation labels shape : %s' % (X_val.shape, Y_val.shape))\n",
    "print ('Test data shape : %s,  Test labels shape : %s' % (X_te.shape, Y_te.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data is successfully loaded into memory, above cell print this message. If not, check the path to directory.\n",
    "\n",
    "    Train data shape : (49000, 3073),  Train labels shape : (49000,)\n",
    "    Validatoin data shape : (1000, 3073),  Validation labels shape : (1000,)\n",
    "    Test data shape : (1000, 3073),  Test labels shape : (1000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "images_index = np.int32(np.round(np.random.rand(18,)*49000,0))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for ax, idx in zip(axes.flat, images_index):\n",
    "    img = (X_tr[idx,:3072].reshape(32, 32, 3) + mean_img.reshape(32, 32, 3))/255.\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[Y_tr[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement two version of loss functions for softmax classifier, and test it out on the CIFAR10 dataset.\n",
    "\n",
    "First, implement the naive softmax loss function with nested loops.\n",
    "\n",
    "You should complete naive_softmax_loss function in YourAnswer.py file line number 117.\n",
    "\n",
    "Generate a random softmax weight matrix and use it to compute the loss. As a rough sanity check, our loss should be something close to -log(0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = naive_softmax_loss(W, X_tr, Y_tr, 0.0)\n",
    "\n",
    "print ('loss :', loss)\n",
    "print ('sanity check : ', -np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is the vectorized softmax loss function. You can implement this function without loop statements.\n",
    "\n",
    "Compare two versions. The two versions should compute the same results, but the vectorized version should be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_time = time.time()\n",
    "loss_naive, grad_naive = naive_softmax_loss(W, X_tr, Y_tr, 0.00001)\n",
    "print ('naive loss : %e with %fs' % (loss_naive, time.time()-s_time))\n",
    "\n",
    "s_time = time.time()\n",
    "loss_vectorized, grad_vectorized = vectorized_softmax_loss(W, X_tr, Y_tr, 0.00001)\n",
    "print ('vectorized loss : %e with %fs' % (loss_vectorized, time.time()-s_time))\n",
    "\n",
    "print ('loss difference : %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print ('gradient difference : %f' % np.linalg.norm(grad_naive-grad_vectorized, ord='fro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should implement the softmax classifier with softmax loss function you implemented above. go to YourFunction.py file and complete Softmax class.\n",
    "\n",
    "Use the validatoin set to tune hyperparemeters (regularizatoin strength and learning rate).\n",
    "You should experiment with different range for the learning rates and regularization strength;\n",
    "if you are careful you should be able to get a classification accuracy of over 0.35 on the validatoin set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results is dictionary mapping tuples of the form.\n",
    "# (learning_rate, regularization_strength) to tuple of the form (training_accuracy, validation_accuracy).\n",
    "# The accuracy is simply the fraction of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-8, 1e-7, 5e-7, 1e-6]\n",
    "regularization_strengths = [5e2, 1e3, 1e4, 5e4]\n",
    "train_acc = 0\n",
    "val_acc = 0\n",
    "#########################################################################################################\n",
    "# TODO : Write code that chooses the best hyperparameters by tuning on the validation set.              # \n",
    "#        For each combination of hyperparemeters, train a Softmax on the training set,                  #\n",
    "#        compute its accuracy on the training and validatoin sets, and store these numbers in the       #\n",
    "#        results dictionary. In addition, store the best validation accuracy in best_val                #\n",
    "#        and the Softmax object that achieves this accuracy in best_softmax.                            #\n",
    "#                                                                                                       #\n",
    "# Hint : You should use a small value for num_iters as you develop your validation code so that the     #\n",
    "#        Softmax don't take much time to train; once you are confident that your validation code works, #\n",
    "#        you should rerun the validation code with a larger value for num_iter.                         #\n",
    "\n",
    "#softmax = Softmax()\n",
    "        \n",
    "for l_rate in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "#------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "\n",
    "#-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "#########################################################################################################\n",
    "        results[(l_rate,reg)] = (train_acc, val_acc)\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print ('lr %e reg %e train accuracy : %f, val accuracy : %f ' % (lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print ('best validatoin accuracy achieved during cross-validation :', best_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best softmax on testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_te_pred = best_softmax.predict(X_te)\n",
    "test_accuracy = np.mean(Y_te == Y_te_pred)\n",
    "\n",
    "print ('softmax on raw pixels final test set accuracy : ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize (Image, Predicted label) pairs of the best softmax model. Results may are not good because we train simple softmax classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "images_index = np.int32(np.round(np.random.rand(18,)*1000,0))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for ax, idx in zip(axes.flat, images_index):\n",
    "    img = (X_te[idx,:3072].reshape(32, 32, 3) + mean_img.reshape(32, 32, 3))/255.\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[Y_te_pred[idx]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the learned weights for each class. Depending on your choice of learning rate and regularization strength, these may or may not be nice to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = best_softmax.Weights[:-1, :]\n",
    "w = w.reshape(32,32,3,10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    \n",
    "    wimg=255.0*(w[:,:,:,i].squeeze() - w_min)/(w_max-w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
