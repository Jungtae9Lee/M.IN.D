{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1. Softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as pickle\n",
    "import time\n",
    "\n",
    "# set default plot options\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up input preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to download the data from the CIFAR-10 website. To do this, simply run following command, and then load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the input dataset.\n",
    "# !./get_cifar10.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of CIFAR10 dataset.\n",
    "def load_CIFAR10(root):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(root, 'data_batch_%d' % (b, ))\n",
    "        with open(f, 'rb') as f:\n",
    "            datadict = pickle.load(f, encoding='latin1')\n",
    "            X = datadict['data']\n",
    "            Y = datadict['labels']\n",
    "            X = X.reshape(10000,3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
    "            Y = np.array(Y)\n",
    "        #X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X,Y\n",
    "    \n",
    "    f=os.path.join(root, 'test_batch')\n",
    "    with open(f, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding='latin1')\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        Xte = X.reshape(10000,3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Yte = np.array(Y)\n",
    "        \n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cifar-10-batches-py\\\\data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9d825088fb61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_CIFAR10_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Train data shape : %s,  Train labels shape : %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Validatoin data shape : %s,  Validation labels shape : %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-9d825088fb61>\u001b[0m in \u001b[0;36mget_CIFAR10_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_CIFAR10_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# 1. Load the raw data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_CIFAR10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./cifar-10-batches-py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# 2. Divide the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a985cc712ce0>\u001b[0m in \u001b[0;36mload_CIFAR10\u001b[1;34m(root)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data_batch_%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mdatadict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cifar-10-batches-py\\\\data_batch_1'"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data():\n",
    "    # 1. Load the raw data\n",
    "    X_tr, Y_tr, X_te, Y_te = load_CIFAR10('./cifar-10-batches-py')\n",
    "    \n",
    "    # 2. Divide the data\n",
    "    X_val, Y_val = X_tr[49000:], Y_tr[49000:]\n",
    "    X_tr, Y_tr = X_tr[:49000], Y_tr[:49000]\n",
    "    X_te, Y_te = X_te[:1000], Y_te[:1000]\n",
    "\n",
    "    # 3. Preprocess the input image\n",
    "    X_tr = np.reshape(X_tr, (X_tr.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0],-1))\n",
    "    X_te = np.reshape(X_te, (X_te.shape[0],-1))\n",
    "    \n",
    "    # 4. Normalize the data (subtract the mean image)\n",
    "    mean_img = np.mean(X_tr, axis = 0)\n",
    "    X_tr -= mean_img\n",
    "    X_val -= mean_img\n",
    "    X_te -= mean_img\n",
    "\n",
    "    # 5. Add bias and Transform into columns\n",
    "    X_tr = np.hstack([X_tr, np.ones((X_tr.shape[0],1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0],1))])\n",
    "    X_te = np.hstack([X_te, np.ones((X_te.shape[0],1))])\n",
    "    \n",
    "    return X_tr, Y_tr, X_val, Y_val, X_te, Y_te, mean_img\n",
    "\n",
    "\n",
    "\n",
    "X_tr, Y_tr, X_val, Y_val, X_te, Y_te, mean_img = get_CIFAR10_data()\n",
    "print ('Train data shape : %s,  Train labels shape : %s' % (X_tr.shape, Y_tr.shape))\n",
    "print ('Validatoin data shape : %s,  Validation labels shape : %s' % (X_val.shape, Y_val.shape))\n",
    "print ('Test data shape : %s,  Test labels shape : %s' % (X_te.shape, Y_te.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-884ab7585752>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3072\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmean_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mY_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_tr' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "images_index = np.int32(np.round(np.random.rand(18,)*49000,0))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for ax, idx in zip(axes.flat, images_index):\n",
    "    img = (X_tr[idx,:3072].reshape(32, 32, 3) + mean_img.reshape(32, 32, 3))/255.\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[Y_tr[idx]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement two version of loss functions for softmax classifier, and test it out on the CIFAR10 dataset.\n",
    "\n",
    "First, implement the naive softmax loss function with nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax_loss(Weights,X_data,Y_data,reg):\n",
    "    \"\"\"\n",
    "     Inputs have D dimension, there are C classes, and we operate on minibatches of N examples.\n",
    "    \n",
    "     Inputs :\n",
    "         - Weights : A numpy array of shape (D,C) containing weights.\n",
    "         - X_data : A numpy array of shape (N,D) contatining a minibatch of data.\n",
    "         - Y_data : A numpy array of shape (N,) containing training labels; \n",
    "               Y[i]=c means that X[i] has label c, where 0<=c<C.\n",
    "         - reg : Regularization strength. (float)\n",
    "         \n",
    "     Returns :\n",
    "         - loss as single float\n",
    "         - gradient with respect to Weights; an array of sample shape as Weights\n",
    "     \"\"\"\n",
    "    \n",
    "    # Initialize the loss and gradient to zero\n",
    "    softmax_loss = 0.0\n",
    "    dWeights = np.zeros_like(Weights)\n",
    "    \n",
    "    ####################################################################################################\n",
    "    # TODO : Compute the softmax loss and its gradient using explicit loops.                           # \n",
    "    #        Store the loss in loss and the gradient in dW.                                            #\n",
    "    #        If you are not careful here, it is easy to run into numeric instability.                  #\n",
    "    #        Don't forget the regularization.                                                          #\n",
    "    #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "    \n",
    "    #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "    ####################################################################################################\n",
    "    return softmax_loss, dWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random softmax weight matrix and use it to compute the loss. As a rough sanity check, our loss should be something close to -log(0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-80b04940b497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3073\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaive_softmax_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'loss :'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'sanity check : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_tr' is not defined"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = naive_softmax_loss(W, X_tr, Y_tr, 0.0)\n",
    "\n",
    "print ('loss :', loss)\n",
    "print ('sanity check : ', -np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is the vectorized softmax loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_softmax_loss(Weights, X_data, Y_data, reg):\n",
    "    softmax_loss = 0.0\n",
    "    dWeights = np.zeros_like(Weights)\n",
    "\n",
    "    ####################################################################################################\n",
    "    # TODO : Compute the softmax loss and its gradient using no explicit loops.                        # \n",
    "    #        Store the loss in loss and the gradient in dW.                                            #\n",
    "    #        If you are not careful here, it is easy to run into numeric instability.                  #\n",
    "    #        Don't forget the regularization.                                                          #\n",
    "    #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "    \n",
    "    #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "    ####################################################################################################\n",
    "    \n",
    "    return softmax_loss, dWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare two versions. The two versions should compute the same results, but the vectorized version should be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f7f6ce162618>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ms_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss_naive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_naive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaive_softmax_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'naive loss : %e with %fs'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss_naive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ms_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ms_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_tr' is not defined"
     ]
    }
   ],
   "source": [
    "s_time = time.time()\n",
    "loss_naive, grad_naive = naive_softmax_loss(W, X_tr, Y_tr, 0.00001)\n",
    "print ('naive loss : %e with %fs' % (loss_naive, time.time()-s_time))\n",
    "\n",
    "s_time = time.time()\n",
    "loss_vectorized, grad_vectorized = vectorized_softmax_loss(W, X_tr, Y_tr, 0.00001)\n",
    "print ('vectorized loss : %e with %fs' % (loss_vectorized, time.time()-s_time))\n",
    "\n",
    "print ('loss difference : %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print ('gradient difference : %f' % np.linalg.norm(grad_naive-grad_vectorized, ord='fro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should implement the softmax classifier using the comment below with softmax loss function you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    def __init__(self):\n",
    "        #self.Weights = None\n",
    "        return\n",
    "        \n",
    "    def train(self, X_tr_data, Y_tr_data, X_val_data, Y_val_data, lr=1e-3, reg=1e-5, iterations=100, bs=128, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this Softmax classifier using stochastic gradient descent.\n",
    "        \n",
    "        Inputs have D dimensions, and we operate on N examples.\n",
    "        \n",
    "        Inputs :\n",
    "            - X_data : A numpy array of shape (N,D) containing training data.\n",
    "            - Y_data : A numpy array of shape (N,) containing training labels;\n",
    "                  Y[i]=c means that X[i] has label 0<=c<C for C classes.\n",
    "            - lr : (float) Learning rate for optimization.\n",
    "            - reg : (float) Regularization strength. \n",
    "            - iterations : (integer) Number of steps to take when optimizing. \n",
    "            - bs : (integer) Number of training examples to use at each step.\n",
    "            - verbose : (boolean) If true, print progress during optimization.\n",
    "        \n",
    "        Regurns :\n",
    "            - A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_train, dim = X_tr_data.shape\n",
    "        num_classes = np.max(Y_tr_data)+1\n",
    "        self.Weights = 0.001*np.random.randn(dim, num_classes)\n",
    "            \n",
    "        for it in range(iterations):\n",
    "            #X_batch = None\n",
    "            #Y_batch = None\n",
    "            \n",
    "            ####################################################################################################\n",
    "            # TODO : Sample batch_size elements from the training data and their corresponding labels          #\n",
    "            #        to use in this round of gradient descent.                                                 #\n",
    "            #        Store the data in X_batch and their corresponding labels in Y_batch; After sampling       #\n",
    "            #        X_batch should have shape (dim, batch_size) and Y_batch should have shape (batch_siae,)   #\n",
    "            #                                                                                                  #\n",
    "            #        Hint : Use np.random.choice to generate indicies.                                         #\n",
    "            #               Sampling with replacement is faster than sampling without replacement.             #\n",
    "            #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "            \n",
    "            #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "            ####################################################################################################\n",
    "\n",
    "            # Evaluate loss and gradient\n",
    "            tr_loss, tr_grad = self.loss(X_batch, Y_batch, reg)\n",
    "\n",
    "            # Perform parameter update\n",
    "            ####################################################################################################\n",
    "            # TODO : Update the weights using the gradient and the learning rate                               #\n",
    "            #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "\n",
    "            #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "            ####################################################################################################\n",
    "\n",
    "            if verbose and it % num_iters == 0:\n",
    "                print ('Ieration %d / %d : loss %f ' % (it, num_iters, loss))\n",
    "            \n",
    "        \n",
    "    \n",
    "    def predict(self, X_data):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this softmax classifier to predict labels for data points.\n",
    "        \n",
    "        Inputs :\n",
    "            - X : A numpy array of shape (N,D) containing training data.\n",
    "            \n",
    "        Returns :\n",
    "             - Y_pred : Predicted labels for the data in X. Y_pred is a 1-dimensional array of length N, \n",
    "                        and each element is an integer giving the predicted class.\n",
    "        \"\"\"\n",
    "        Y_pred = np.zeros(X_data.shape[0])\n",
    "        \n",
    "        ####################################################################################################\n",
    "        # TODO : Implement this method. Store the predicted labels in Y_pred                               #\n",
    "        #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "\n",
    "        #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "        ####################################################################################################\n",
    "        return Y_pred\n",
    "    \n",
    "    def get_accuracy(self, X_data, Y_data):\n",
    "        \"\"\"\n",
    "        Use X_data and Y_data to get an accuracy of the model.\n",
    "        \n",
    "        Inputs :\n",
    "            - X_data : A numpy array of shape (N,D) containing input data.\n",
    "            - Y_data : A numpy array of shape (N,) containing a true label.\n",
    "            \n",
    "        Returns :\n",
    "             - Accuracy : Accuracy of input data pair [X_data, Y_data].\n",
    "        \"\"\"\n",
    "        ####################################################################################################\n",
    "        # TODO : Implement this method. Calculate an accuracy of X_data using Y_data and predict Func                               #\n",
    "        #---------------------------------------WRITE YOUR CODE--------------------------------------------#\n",
    "\n",
    "        #--------------------------------------END OF YOUR CODE--------------------------------------------#\n",
    "        ####################################################################################################\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def loss(self, X_batch, Y_batch, reg):\n",
    "        return vectorized_softmax_loss(self.Weights, X_batch, Y_batch, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the validatoin set to tune hyperparemeters (regularizatoin strength and learning rate).\n",
    "You should experiment with different range for the learning rates and regularization strength;\n",
    "if you are careful you should be able to get a classification accuracy of over 0.35 on the validatoin set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validatoin accuracy achieved during cross-validation : -1\n"
     ]
    }
   ],
   "source": [
    "# results is dictionary mapping tuples of the form.\n",
    "# (learning_rate, regularization_strength) to tuple of the form (training_accuracy, validation_accuracy).\n",
    "# The accuracy is simply the fraction of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-8, 1e-7, 5e-7, 1e-6]\n",
    "regularization_strengths = [5e2, 1e3, 1e4, 5e4]\n",
    "\n",
    "#########################################################################################################\n",
    "# TODO : Write code that chooses the best hyperparameters by tuning on the validation set.              # \n",
    "#        For each combination of hyperparemeters, train a Softmax on the training set,                  #\n",
    "#        compute its accuracy on the training and validatoin sets, and store these numbers in the       #\n",
    "#        results dictionary. In addition, store the best validation accuracy in best_val                #\n",
    "#        and the Softmax object that achieves this accuracy in best_softmax.                            #\n",
    "#                                                                                                       #\n",
    "# Hint : You should use a small value for num_iters as you develop your validation code so that the     #\n",
    "#        Softmax don't take much time to train; once you are confident that your validation code works, #\n",
    "#        you should rerun the validation code with a larger value for num_iter.                         #\n",
    "#------------------------------------------WRITE YOUR CODE----------------------------------------------#\n",
    "\n",
    "#softmax = Softmax() #Use Softmax() class!\n",
    "\n",
    "#-----------------------------------------END OF YOUR CODE----------------------------------------------#\n",
    "#########################################################################################################\n",
    "\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print ('lr %e reg %e train accuracy : %f, val accuracy : %f ' % (lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print ('best validatoin accuracy achieved during cross-validation :', best_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best softmax on testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_te_pred = best_softmax.predict(X_te)\n",
    "test_accuracy = np.mean(Y_te == Y_te_pred)\n",
    "\n",
    "print ('softmax on raw pixels final test set accuracy : ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize (Image, Predicted label) pairs of the best softmax model. Results may are not good because we train simple softmax classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "images_index = np.int32(np.round(np.random.rand(18,)*1000,0))\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 6),\n",
    "                         subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for ax, idx in zip(axes.flat, images_index):\n",
    "    img = (X_te[idx,:3072].reshape(32, 32, 3) + mean_img.reshape(32, 32, 3))/255.\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[Y_te_pred[idx]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the learned weights for each class. Depending on your choice of learning rate and regularization strength, these may or may not be nice to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = best_softmax.Weights[:-1, :]\n",
    "w = w.reshape(32,32,3,10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    \n",
    "    wimg=255.0*(w[:,:,:,i].squeeze() - w_min)/(w_max-w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
